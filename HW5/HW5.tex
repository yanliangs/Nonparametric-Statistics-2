\documentclass[letterpaper,11pt]{article}
\usepackage{latexsym}
\usepackage[empty]{fullpage}
\usepackage[usenames,dvipsnames]{color}
\usepackage{verbatim}
\usepackage{hyperref}
\usepackage{framed}
\usepackage{tocloft}
\usepackage{bibentry}
\usepackage{amsmath}
\usepackage{scrextend}
\usepackage{listings}
\usepackage{color}
\usepackage{fancyhdr}

%THIS PORTION IS FOR ADDING PAGE NUMBER
\pagestyle{fancy} 
\cfoot{}
\rfoot{\thepage}
\renewcommand{\headrulewidth}{0pt}
%THIS PORTION IS FOR ADDING PAGE NUMBER



\urlstyle{same}
\definecolor{mygrey}{gray}{.85}
\definecolor{mygreylink}{gray}{.30}
\textheight=9.0in
\raggedbottom
\raggedright
\setlength{\tabcolsep}{0in}


%The following part is for inserting codes in LaTeX:
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
 
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstset{style=mystyle}
%For inserting codes in LaTeX




\begin{document}
\begin{addmargin}[-2em]{0em} \large{\textbf{1. }}\end{addmargin}

Since $g$ is strictly increasing, then we have:
$$p = p_{perm} = \frac{1}{\begin{pmatrix}
m+n\\
n 
\end{pmatrix}}\sum_{i=1}^{\begin{pmatrix}
m+n\\
n
\end{pmatrix}}\mathbb{1}(D_i\geq D_{obs})$$

\begin{addmargin}[-2em]{0em} \large{\textbf{2. }}\end{addmargin}
 
We know the fact that  $$\bar{X} - \bar{Y} \sim N(0, \sigma_y^2/m )$$  and 
 
$$ \sqrt{\frac{s_x^2}{n} +\frac{s_y^2}{m}} \approx \sqrt{\frac{s_y^2}{m}} \sim \frac{\sigma}{\sqrt{m}} \sqrt{\frac{\chi^2_{m-1}}{m-1}} $$
 
Hence, 
 
$$0.02 = \mathbb{P} \left( \frac{|\bar{X} - \bar{Y}|}{\sqrt{\frac{s_x^2}{n} +\frac{s_y^2}{m}}} > T|H_0 \right) \approx \mathbb{P}(|t_{m-1}|>T|H_0)$$
 
Then we measure the probability of Type I error of the resulting test by employing Monte Carlo simulation.
 
\begin{enumerate}
\item[1.] For a given $\sigma_x$ and $\sigma_y$ , generate 5 samples from $N(0, \sigma^2_x)$ and 10 samples from $N(0, \sigma^2_y)$. Calculate the statistic $T$ for this sample.
 
\item[2.] Repeat this 10000 times and obtain $T_1, T_2, \ldots , T_{10000}$.
 
\item[3.] Estimate the probability of Type I error as $\frac{1}{10000} \sum^{10000}_{i=1} \mathbb{I}(T_i > 4.606)$.
\end{enumerate}
The codes are as followsï¼š

\begin{lstlisting}
	
\end{lstlisting}

 
To estimate the distribution we give the plot the empirical CDF of $T_1, T_2, \ldots , T_{10000}$:

\begin{lstlisting}
	
\end{lstlisting}




\begin{addmargin}[-2em]{0em} \large{\textbf{3. }}\end{addmargin}


\begin{lstlisting}
clc; clear all;

p    = 0.99;

sig1 = 1; 
mu1  = 0;

sig2 = 5;
mu2  = 0;

T    = [10 100 250 500 1000];

mu=zeros(length(T),1);
CI=zeros(2,length(T));

muboot=zeros(length(T),1);
CIboot=zeros(2,length(T));
CIboot2=zeros(2,length(T));

for k = 1:length(T)
    rng(2)
    B         = rand(T(k),1);
    id_B      =  B < p;
    Y         = id_B .* ( mu1 + sig1 * randn( T(k), 1 ) ) + (1-id_B) .* ( mu2 + sig2 * randn( T(k), 1 ) );
    mu(k)     = mean( Y );
    SE(k)     = std( Y )/sqrt( T(k) );
    
    CI(:,k)   = [mu(k) + tinv(0.025,T(k)-1) * SE(k), mu(k) + tinv(0.975,T(k)-1) * SE(k)];
    
    id        = randi(T(k),T(k),10000);
    mubootvec = zeros(10000,1);
    Zboot     = zeros(size(Y));
    
    for i=1:size(id,2)
        Yboot = Y(id(:,i));
        mubootvec(i) = mean(Yboot);
        SEboot_i =std(Yboot)/sqrt(T(k));
        Zboot(i) = ( mean(Yboot) - mu(k) ) / SEboot_i; %pivot
    end
    muboot(k) = mean(mubootvec);
    SEboot(k) = std(mubootvec);
   
    CIboot(:,k) = [mu(k) - quantile(Zboot,0.975) * SEboot(k), mu(k) - quantile(Zboot,0.025) * SEboot(k)];
    CIboot2(:,k) = [quantile(mubootvec,0.025), quantile(mubootvec,0.975)];
    
    disp(num2str(k))
end
disp([' p = ',num2str(p)])
disp(' ')
disp(' E[Y]:')
disp(num2str(mu))
disp(' ')
disp(' Bootstrap E[Y]:')
disp(num2str(muboot))
disp(' ')
disp('CI:')
disp(num2str(CI))
disp(' ')
disp('CIboot: ')
disp(num2str(CIboot))
disp(' ')
disp('SE: ')
disp(SE)
disp(' ')
disp('SEboot: ')
disp(SEboot)
\end{lstlisting}

The results are:

\begin{lstlisting}
p = 0.99
 
 E[Y]:
 -0.17864
-0.088922
 -0.05622
-0.051865
-0.042969
 
 Bootstrap E[Y]:
 -0.17964
-0.088048
-0.056585
-0.051159
-0.042382
 
CI:
-0.98413    -0.28421    -0.18434    -0.15998    -0.10951
 0.62685     0.10637    0.071902    0.056247    0.023577
 
CIboot: 
-1.5367    -0.28292    -0.18486    -0.17177    -0.10984
0.42867    0.098969    0.072752    0.047057    0.021615
 
SE: 
    0.3561    0.0984    0.0651    0.0550    0.0339

 
SEboot: 
    0.3370    0.0967    0.0651    0.0551    0.0339
\end{lstlisting}

\bigbreak


\begin{center}
\begin{tabular}{ |c|c|c|c|c|c| }
 \hline
 \multicolumn{6}{|c|}{Results Under $p=0.99$} \\
 \hline
number of sample & mean of Y & 95\% CI & standard error & bootstrap CI & bootstrap se\\
 \hline
 10  & -0.17864    &(-0.98413,0.62685)&0.3561 &(-1.5367,0.42867)& 0.3370\\
 100  & -0.088922  &(-0.282421,0.10637)&0.3561 &(-0.28292,0.098969)&0.0967\\
 250 &-0.05622 &(-0.18434,0.071902)& 0.0651&(-0.18486,0.072752)&0.0651\\
 500    & -0.051865 &(-0.15998,0.056247)&0.0550 &(-0.17177,0.047057)&0.0551\\
 1000 &-0.042969 &(-0.10951,0.023577)& 0.0339&(-0.10984,0.021615)&0.0339\\
 \hline
\end{tabular}
\end{center}


Similarly, by changing the parameter into $p=0.95$, we get the following results:

\begin{lstlisting}
	p = 0.95
 
 E[Y]:
 -0.17864
  -0.2312
-0.080097
-0.071739
-0.040275
 
 Bootstrap E[Y]:
 -0.17964
 -0.22993
-0.080617
-0.071372
-0.039482
 
CI:
-0.98413    -0.47879    -0.23961    -0.21885    -0.13636
 0.62685    0.016388    0.079413    0.075367    0.055811
 
CIboot: 
-1.5367    -0.49715    -0.23278    -0.22825    -0.13435
0.42867   -0.012391    0.076686    0.066612     0.05359
 
SE: 
    0.3561    0.1248    0.0810    0.0749    0.0490

 
SEboot: 
    0.3370    0.1224    0.0798    0.0745    0.0483
\end{lstlisting}

\begin{center}
\begin{tabular}{ |c|c|c|c|c|c| }
 \hline
 \multicolumn{6}{|c|}{Results Under $p=0.95$} \\
 \hline
number of sample & mean of Y & 95\% CI & standard error & bootstrap CI & bootstrap se\\
 \hline
 10  & -0.17864    &(-0.98413,0.62685)&0.3561 &(-1.5367,0.42867)& 0.3370\\
 100  & -0.2312  &(-0.47879,0.016388)&0.1248 &(-0.49715,-0.012391)&0.1224\\
 250 &-0.080097 &(-0.23961,0.079413)& 0.0810&(-0.23278,0.076686)&0.0798\\
 500    & -0.071739 &(-0.21885,0.075367)&0.0749 &(-0.22825,0.066612)&0.0745\\
 1000 &-0.040275 &(-0.13636,0.055811)& 0.0490&(-0.13435,0.05359)&0.0483\\
 \hline
\end{tabular}
\end{center}


According to the results, we can see that contamination on the standard error would reduce the accuracy of the confidence interval.
\newpage

\begin{addmargin}[-2em]{0em} \large{\textbf{4. }}\end{addmargin}

Given that the parameters $\sigma_x=\sigma_y=1$ and 
$$\rho_{xy}=\frac{\sigma_{xy}}{\sigma_x\sigma_y}$$

we have the covariance matrix:

$$\begin{pmatrix}
1,\rho_{xy}\\
\rho_{xy},1
\end{pmatrix}$$

then under the following 3 scenarios, we do bootstrap for a sample of 10, 250 and 1000, each for 10000 times in otder to have stable results.
\bigbreak
\begin{addmargin}[-1.1em]{0em} \textbf{(i)}$\rho_{xy}=0.99$\par \end{addmargin}


\begin{addmargin}[0em]{0em} \textbf{(1)}n=10\par \end{addmargin}

\begin{lstlisting}
n<-10
boot<-rep(0,10000)
mu<-c(0,0)
sigma<-matrix(c(1,0.99,0.99,1),nrow=2)
sample <- mvrnorm(n,mu,sigma)

	rho <- rep(0,10000)
for (i in 1:10000){
  order<-sample(1:n,n,replace=T)
  x<-sample[order,1]
  y<-sample[order,2]
  rho[i]<-cov(x,y)/(var(x)*var(y))^0.5
}
\end{lstlisting}
The results are:
\begin{lstlisting}
> quantile(rho,0.025)
     2.5% 
0.9884495 
> quantile(rho,0.975)
    97.5% 
0.9993013 
\end{lstlisting}

\begin{addmargin}[0em]{0em} \textbf{(2)}n=250\par \end{addmargin}
\begin{lstlisting}
	n<-250
sample <- mvrnorm(n,mu,sigma)

#define the vector that stores all the 10000 bootsrap velues
rho <- rep(0,10000)
for (i in 1:10000){
  order<-sample(1:n,n,replace=T)
  x<-sample[order,1]
  y<-sample[order,2]
  rho[i]<-cov(x,y)/(var(x)*var(y))^0.5
}
\end{lstlisting}
The results are:
\begin{lstlisting}
> quantile(rho,0.025)
     2.5% 
0.9888638 
> quantile(rho,0.975)
    97.5% 
0.9928094 
\end{lstlisting}

\begin{addmargin}[0em]{0em} \textbf{(3)}n=1000\par \end{addmargin}
\begin{lstlisting}
	n<-1000
sample <- mvrnorm(n,mu,sigma)

#define the vector that stores all the 10000 bootsrap velues
rho <- rep(0,10000)
for (i in 1:10000){
  order<-sample(1:n,n,replace=T)
  x<-sample[order,1]
  y<-sample[order,2]
  rho[i]<-cov(x,y)/(var(x)*var(y))^0.5
}
\end{lstlisting}
The results are:
\begin{lstlisting}
> quantile(rho,0.025)
     2.5% 
0.9874501 
> quantile(rho,0.975)
    97.5% 
0.9902186 
\end{lstlisting}





\begin{addmargin}[-1.1em]{0em} \textbf{(ii)}$\rho_{xy}=-0.5$\par \end{addmargin}


\begin{addmargin}[0em]{0em} \textbf{(1)}n=10\par \end{addmargin}
\begin{lstlisting}
	boot<-rep(0,10000)
mu<-c(0,0)
sigma<-matrix(c(1,-0.5,-0.5,1),nrow=2)
n<-10
sample <- mvrnorm(n,mu,sigma)
#define the vector that stores all the 10000 bootsrap velues
rho <- rep(0,10000)
for (i in 1:10000){
  order<-sample(1:n,n,replace=T)
  x<-sample[order,1]
  y<-sample[order,2]
  rho[i]<-cov(x,y)/(var(x)*var(y))^0.5
}
\end{lstlisting}
The results are:
\begin{lstlisting}
> quantile(rho,0.025)
      2.5% 
-0.8663871 
> quantile(rho,0.975)
      97.5% 
-0.07224249 
\end{lstlisting}

\begin{addmargin}[0em]{0em} \textbf{(2)}n=250\par \end{addmargin}
\begin{lstlisting}
n<-250
	sample <- mvrnorm(n,mu,sigma)
#define the vector that stores all the 10000 bootsrap velues
rho <- rep(0,10000)
for (i in 1:10000){
  order<-sample(1:n,n,replace=T)
  x<-sample[order,1]
  y<-sample[order,2]
  rho[i]<-cov(x,y)/(var(x)*var(y))^0.5
}
\end{lstlisting}
The results are:
\begin{lstlisting}
> quantile(rho,0.025)
      2.5% 
-0.5671849 
> quantile(rho,0.975)
     97.5% 
-0.3659588 
\end{lstlisting}

\begin{addmargin}[0em]{0em} \textbf{(3)}n=1000\par \end{addmargin}
\begin{lstlisting}
	n<-1000
sample <- mvrnorm(n,mu,sigma)
#define the vector that stores all the 10000 bootsrap velues
rho <- rep(0,10000)
for (i in 1:10000){
  order<-sample(1:n,n,replace=T)
  x<-sample[order,1]
  y<-sample[order,2]
  rho[i]<-cov(x,y)/(var(x)*var(y))^0.5
}
\end{lstlisting}
The results are:
\begin{lstlisting}
> quantile(rho,0.025)
      2.5% 
-0.5707218 
> quantile(rho,0.975)
     97.5% 
-0.4811284 
\end{lstlisting}



\begin{addmargin}[-1.1em]{0em} \textbf{(iii)}$\rho_{xy}=0$\par \end{addmargin}


\begin{addmargin}[0em]{0em} \textbf{(1)}n=10\par \end{addmargin}
\begin{lstlisting}
	boot<-rep(0,10000)
mu<-c(0,0)
sigma<-matrix(c(1,0,0,1),nrow=2)
n<-10
sample <- mvrnorm(n,mu,sigma)
#define the vector that stores all the 10000 bootsrap velues
rho <- rep(0,10000)
for (i in 1:10000){
  order<-sample(1:n,n,replace=T)
  x<-sample[order,1]
  y<-sample[order,2]
  rho[i]<-cov(x,y)/(var(x)*var(y))^0.5
}
\end{lstlisting}
The results are:
\begin{lstlisting}
> quantile(rho,0.025)
      2.5% 
-0.8604943 
> quantile(rho,0.975)
    97.5% 
0.5113225 
\end{lstlisting}

\begin{addmargin}[0em]{0em} \textbf{(2)}n=250\par \end{addmargin}
\begin{lstlisting}
n<-250
	sample <- mvrnorm(n,mu,sigma)
#define the vector that stores all the 10000 bootsrap velues
rho <- rep(0,10000)
for (i in 1:10000){
  order<-sample(1:n,n,replace=T)
  x<-sample[order,1]
  y<-sample[order,2]
  rho[i]<-cov(x,y)/(var(x)*var(y))^0.5
}
\end{lstlisting}
The results are:
\begin{lstlisting}
> quantile(rho,0.025)
      2.5% 
-0.8604943 
> quantile(rho,0.975)
    97.5% 
0.5113225 
\end{lstlisting}

\begin{addmargin}[0em]{0em} \textbf{(3)}n=1000\par \end{addmargin}
\begin{lstlisting}
	n<-1000
sample <- mvrnorm(n,mu,sigma)
#define the vector that stores all the 10000 bootsrap velues
rho <- rep(0,10000)
for (i in 1:10000){
  order<-sample(1:n,n,replace=T)
  x<-sample[order,1]
  y<-sample[order,2]
  rho[i]<-cov(x,y)/(var(x)*var(y))^0.5
}
\end{lstlisting}
The results are:
\begin{lstlisting}
> quantile(rho,0.025)
       2.5% 
-0.06429994 
> quantile(rho,0.975)
     97.5% 
0.05973886 
\end{lstlisting}


From the results shown above we can see that when $\rho_{xy}$ is fixed, the greater the sample size, the smaller the confidence interval is.

\end{document}







